\documentclass[12pt]{article}

\usepackage[top=1in,bottom=1.5in,left=1in,right=1in]{geometry}
\usepackage{amsmath,amssymb}
\allowdisplaybreaks
\usepackage[colorlinks=true,allcolors=magenta]{hyperref}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\abs}{abs}
\DeclareMathOperator{\diag}{diag}
\newcommand{\transpose}[1]{#1^{\mathrm{T}}}

\begin{document}
\setlength{\parskip}{5mm}
\setlength{\parindent}{0mm}

Let $S$ be an $n \times n$ real positive definite matrix.  Let $L$ be
the unique unit lower triangular matrix satisfying $S = D L L' D$ for
diagonal $D$.  Let $x$ be the vector of length $n (n + 1)/2$ containing
$\{s_{ij}\}_{i \geq j}$ sorted in row major order.  Let $y = f(x)$ be
the vector of length $n (n + 1)/2$ containing $\{l_{ij}\}_{i \geq j}$
sorted in row major order, but with $\log(s_{ii})/2$ replacing $l_{ii}$.

Adopting the notation
$x_{ij} = s_{ij}$,
$y_{ij} = \log(s_{ii}) / 2$ for $i = j$, and
$y_{ij} = l_{ij}$ for $i > j$,
we can write the absolute value of the Jacobian determinant of
$g = f^{-1}$ as
\begin{equation}
\label{eq:absdetjac}
\begin{aligned}
&\abs(\det(\partial g/\partial y)) \\
&= \Big( \prod_{i=1}^{n} \exp(2 y_{ii})^{(n-1)/2} \Big( 1 + \sum_{j=1}^{i-1} y_{ij}^{2} \Big)^{-(n-i)/2} \Big) \Big( \prod_{i=1}^{n} \Big( 1 + \sum_{j=1}^{i-1} y_{ij}^{2} \Big)^{-(i+2)/2} \Big) \Big( \prod_{i=1}^{n} 2 \exp(2 y_{ii}) \Big) \\
&= 2^{n} \exp\Big((n + 1) \sum_{i=1}^{n} y_{ii}\Big) \prod_{i=1}^{n} \Big( 1 + \sum_{j=1}^{i-1} y_{ij}^{2} \Big)^{-(n+2)/2}
\end{aligned}
\end{equation}
and the logarithm as
\begin{equation}
\log(\abs(\det(\partial g/\partial y))) = n \log(2) + \frac{1}{2} (n + 1) \sum_{i=1}^{n} 2 y_{ii} - \frac{1}{2} (n + 2) \sum_{i=1}^{n} \log\Big( 1 + \sum_{j=1}^{n} y_{ij}^{2}\Big)\,.
\end{equation}
Note that $g$ can be written as $g_{3} \circ g_{2} \circ g_{1}$:
\begin{itemize}
\item
$g_{1}$ maps $y_{1} = y$ to $y_{2}$ containing
$\{l_{ij}\}_{i \geq j}$ sorted in row major order with $s_{ii}$
replacing $l_{ii}$;
\item
$g_{2}$ maps $y_{2}$     to $y_{3}$ containing
$\{w_{ij}\}_{i \geq j}$ sorted in row major order with $s_{ii}$
replacing $w_{ii}$, where $W = \diag(\diag(L L'))^{-1/2} L$; and
\item
$g_{3}$ maps $y_{3}$     to $y_{4} = x$.
\end{itemize}
By the chain rule, the Jacobian matrix of $g$ can be written as
\begin{equation}
\label{eq:chainrule}
\frac{\partial g}{\partial y}(y) =
\frac{\partial g_{3}}{\partial y_{3}}(g_{2}(g_{1}(y)))
\frac{\partial g_{2}}{\partial y_{2}}(      g_{1}(y) )
\frac{\partial g_{1}}{\partial y_{1}}(            y  )\,.
\end{equation}
The first equality in Equation~\ref{eq:absdetjac} is the result of
taking $\abs \circ \det$ on each side of Equation~\ref{eq:chainrule}.

Note that the ordering of each $y_{k}$ is chosen so that each
$\partial g_{k}/\partial y_{k}$ has a ``nice'' (typically block
diagonal) structure amenable to the computation of determinants.

\newpage

\section{Correlation matrix distributions}

Let $X$ be an $n \times n$ real symmetric positive definite matrix
with unit diagonal elements (i.e., an $n \times n$ correlation matrix).

\subsection{Lewandowski-Kurowicka-Joe (LKJ) distribution}

If $X$ follows an
\href{https://en.wikipedia.org/wiki/Lewandowski-Kurowicka-Joe_distribution}{LKJ distribution}
with shape $\eta > 0$,
\begin{equation}
X \sim \mathrm{LKJ}(\eta)\,,
\end{equation}
then its probability density is
\begin{equation}
\begin{aligned}
f(X; \eta)
&= 2^{\sum_{j=1}^{n} (2 \eta - 2 + n - j) (n - j)} \det(X)^{\eta - 1} \prod_{j=1}^{n-1} B(\eta + (n - j - 1)/2, \eta + (n - j - 1)/2)^{n-j} \\
&= 2^{n (n - 1) (\eta - 1 + (2 n - 1)/6)} \det(X)^{\eta - 1} \prod_{j=1}^{n-1} \frac{\Gamma(a(j))^{2 j}}{\Gamma(2 a(j))^{j}}
\end{aligned}
\end{equation}
with logarithm
\begin{equation}
\log(f(X; \eta)) = n (n - 1) (\eta - 1 + (2 n - 1)/6) \log(2) + (\eta - 1) \log(\det(X)) + \sum_{j=1}^{n-1} (2 j \log(\Gamma(a(j))) - j \log(\Gamma(2 a(j))))
\end{equation}
where $a(j) = \eta - 1 + (j + 1)/2$.

\subsection{Computation}

$X$ can be factorized as
\begin{equation}
X = D^{-1/2} L \transpose{L} D^{-1/2}\,,
\end{equation}
where $L$ is a lower triangular matrix with unit diagonal elements
and
\begin{equation}
D = \diag(\diag(L \transpose{L}))\,.
\end{equation}
If the normalization constant can be ignored,
then computing $f(X;\eta)$ amounts to computing $\det(X)$:
\begin{equation}
\begin{aligned}
\det(X)
  &= \det(D^{-1/2}) \det(L) \det(\transpose{L}) \det(D^{-1/2}) \\
  &= \det(D)^{-1/2} \cdot 1 \cdot 1 \cdot \det(D)^{-1/2} \\
  &= \det(D)^{-1}\,,
\end{aligned}
\end{equation}
using the fact that
$\det(A B) = \det(A) \det(B)$ for matrices $A$ and $B$
and
$\det(C) = \prod_{k} (C)_{k,k}$ for triangular matrices $C$.


\section{Covariance matrix distributions}

Let $X$ and $S$ be $n \times n$ real symmetric positive definite matrices
(i.e., $n \times n$ covariance matrices), and let $\nu \in (n - 1, \infty)$.

\subsection{Wishart distribution}

If $X$ follows a
\href{https://en.wikipedia.org/wiki/Wishart_distribution}{Wishart distribution}
with shape (degrees of freedom) $\nu$ and scale $S$,
\begin{equation}
X \sim \mathrm{Wishart}(\nu, S)\,,
\end{equation}
then its probability density function is
\begin{equation}
f(X; \nu, S) = \frac{\det(S)^{-\nu/2} \det(X)^{-(-\nu + n + 1)/2}}{2^{n \nu/2} \Gamma_{n}(\nu/2)} \exp\Big(-\frac{1}{2} \tr(S^{-1} X)\Big)
\end{equation}
with logarithm
\begin{equation}
\log(f(X; \nu, S)) = -\frac{1}{2} (\nu \log(\det(S)) + (-\nu + n + 1) \log(\det(X)) + n \nu \log(2) + 2 \log(\Gamma_{n}(\nu/2)) + \tr(S^{-1} X))\,,
\end{equation}
where $\Gamma_{n}$ denotes the
\href{https://en.wikipedia.org/wiki/Multivariate_gamma_function}{$n$-variate gamma function},
given by
\begin{equation}
\Gamma_{n}(z) = \pi^{n (n - 1)/4} \prod_{i=0}^{n-1} \Gamma\Big(z - \frac{i}{2}\Big)\,,\qquad \Re(z) > \frac{1}{2}(n - 1)\,.
\end{equation}

\subsection{Inverse Wishart distribution}

If $X$ follows an
\href{https://en.wikipedia.org/wiki/Inverse-Wishart_distribution}{inverse Wishart distribution}
with shape (degrees of freedom) $\nu$ and scale $S$,
\begin{equation}
X \sim \mathrm{InverseWishart}(\nu, S)\,,
\end{equation}
then its probability density function is
\begin{equation}
f(X; \nu, S) = \frac{\det(S)^{\nu/2} \det(X)^{-(\nu + n + 1)/2}}{2^{n \nu/2} \Gamma_{n}(\nu/2)} \exp\Big(-\frac{1}{2} \tr(S X^{-1})\Big)
\end{equation}
with logarithm
\begin{equation}
\log(f(X; \nu, S)) = -\frac{1}{2} (-\nu \log(\det(S)) + (\nu + n + 1) \log(\det(X)) + n \nu \log(2) + 2 \log(\Gamma_{n}(\nu/2)) + \tr(S X^{-1}))\,.
\end{equation}

\subsection{Computation}

$X$ and $S$ can be factorized as
\begin{equation}
X = D_{\sigma,X} D_{\theta,X}^{-1/2} L_{X} \transpose{L_{X}} D_{\theta,X}^{-1/2} D_{\sigma,X}\,,\qquad S = D_{\sigma,S} D_{\theta,S}^{-1/2} L_{S} \transpose{L_{S}} D_{\theta,S}^{-1/2} D_{\sigma,S}\,,
\end{equation}
where $L_{X}$ and $L_{S}$ are lower triangular matrices
with unit diagonal elements,
\begin{equation}
D_{\sigma,X} = \diag(\diag(X))^{1/2}\,,\qquad D_{\sigma,S} = \diag(\diag(S))^{1/2}\,,
\end{equation}
and
\begin{equation}
D_{\theta,X} = \diag(\diag(L_{X} \transpose{L_{X}}))\,,\qquad D_{\theta,S} = \diag(\diag(L_{S} \transpose{L_{S}}))\,.
\end{equation}

$\det(X)$ is computed as
\begin{equation}
\begin{aligned}
\det(X)
  &= \det(D_{\sigma,X}) \det(D_{\theta,X}^{-1/2}) \det(L_{X}) \det(\transpose{L_{X}}) \det(D_{\theta,X}^{-1/2}) \det(D_{\sigma,X}) \\
  &= \det(D_{\sigma,X}) \det(D_{\theta,X})^{-1/2} \cdot 1 \cdot 1 \cdot \det(D_{\theta,X})^{-1/2} \det(D_{\sigma,X}) \\
  &= \det(D_{\sigma,X})^{2} \det(D_{\theta,X})^{-1}\,,
\end{aligned}
\end{equation}
using the fact that
$\det(A B) = \det(A) \det(B)$ for matrices $A$ and $B$
and
$\det(C) = \prod_{k} (C)_{k,k}$ for triangular matrices $C$.
Similarly,
\begin{equation}
\det(S) = \det(D_{\sigma,S})^{2} \det(D_{\theta,S})^{-1}\,.
\end{equation}

$\tr(S^{-1} X)$ is computed by noting that
\begin{equation}
\tr(A B) = \sum_{i,j} (A \odot \transpose{B})_{i,j} = \sum_{i,j} (\transpose{A} \odot B)_{i,j}\,,
\end{equation}
where $\odot$ denotes elementwise multiplication.
Then
\small
\begin{equation}
\begin{aligned}
\tr(S^{-1} X)
  &= \sum_{i,j} (S^{-1} \odot \transpose{X})_{i,j} \\
  &= \sum_{i,j} (S^{-1} \odot X)_{i,j} \\
  &= \sum_{i,j} (D_{\sigma,S}^{-1} D_{\theta,S}^{1/2} \transpose{(L_{S}^{-1})} L_{S}^{-1} D_{\theta,S}^{1/2} D_{\sigma,S}^{-1} \odot D_{\sigma,X} D_{\theta,X}^{-1/2} L_{X} \transpose{L_{X}} D_{\theta,X}^{-1/2} D_{\sigma,X})_{i,j} \\
  &= \sum_{i,j} (D_{1} V_{1} \transpose{D_{1}})_{i,j}\,,
\end{aligned}
\end{equation}
\normalsize
where
\begin{equation}
V_{1} = \transpose{(L_{S}^{-1})} L_{S}^{-1} \odot L_{X} \transpose{L_{X}}\,,\qquad D_{1} = D_{\sigma,S}^{-1} D_{\theta,S}^{1/2} D_{\sigma,X} D_{\theta,X}^{-1/2}\,.
\end{equation}

Similarly,
\begin{equation}
\begin{aligned}
\tr(S X^{-1})
  &= \sum_{i,j} (\transpose{S} \odot X^{-1})_{i,j} \\
  &= \sum_{i,j} (S \odot X^{-1})_{i,j} \\
  &= \sum_{i,j} (D_{\sigma,S} D_{\theta,S}^{-1/2} L_{S} \transpose{L_{S}} D_{\theta,S}^{-1/2} D_{\sigma,S} \odot D_{\sigma,X}^{-1} D_{\theta,X}^{1/2} \transpose{(L_{X}^{-1})} L_{X}^{-1} D_{\theta,X}^{1/2} D_{\sigma,X}^{-1})_{i,j} \\
  &= \sum_{i,j} (D_{2} V_{2} \transpose{D_{2}})_{i,j}\,,
\end{aligned}
\end{equation}
where
\begin{equation}
V_{2} = L_{S} \transpose{L_{S}} \odot \transpose{(L_{X}^{-1})} L_{X}^{-1}\,,\qquad D_{2} = D_{\sigma,S} D_{\theta,S}^{-1/2} D_{\sigma,X}^{-1} D_{\theta,X}^{1/2}\,.
\end{equation}

\end{document}
