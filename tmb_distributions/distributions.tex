\documentclass[12pt]{article}

\usepackage[top=1in,bottom=1.5in,left=1in,right=1in]{geometry}
\usepackage{amsmath,amssymb}
\allowdisplaybreaks
\usepackage[colorlinks=true,allcolors=magenta]{hyperref}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\newcommand{\transpose}[1]{#1^{\mathrm{T}}}

\begin{document}
\setlength{\parskip}{5mm}
\setlength{\parindent}{0mm}

Let $S$ be an $n \times n$ real positive definite matrix.  Let $L$ be
the unique unit lower triangular matrix satisfying $S = D L L' D$ for
diagonal $D$.  Let $x$ be the vector of length $n (n + 1)/2$ containing
$\{s_{ij}\}_{i \geq j}$ sorted in row major order.  Let $y = f(x)$ be
the vector of length $n (n + 1)/2$ containing $\{l_{ij}\}_{i \geq j}$
sorted in row major order, but with $\log(s_{ii})/2$ replacing $l_{ii}$.

Adopting the notation
$x_{ij} = s_{ij}$,
$y_{ij} = \log(s_{ii}) / 2$ for $i = j$, and
$y_{ij} = l_{ij}$ for $i > j$,
we can write the absolute value of the Jacobian of $g = f^{-1}$ as
\begin{equation}
\begin{aligned}
  &\big|\det(\partial g/\partial y)\big| \\
&= \Big( \prod_{i=1}^{n} 2 \exp(2 y_{ii}) \Big) \Big( \prod_{i=1}^{n} \Big( 1 + \sum_{j=1}^{i-1} y_{ij}^{2} \Big)^{-(i+2)/2} \Big) \Big( \prod_{i=1}^{n} \exp(2 y_{ii})^{(n-1)/2} \Big( 1 + \sum_{j=1}^{i-1} y_{ij}^{2} \Big)^{-(n-i)/2} \Big) \\
&= 2^{n} \exp\Big((n + 1) \sum_{i=1}^{n} y_{ii}\Big) \prod_{i=1}^{n} \Big( 1 + \sum_{j=1}^{i-1} y_{ij}^{2} \Big)^{-(n+2)/2}
\end{aligned}
\end{equation}
and the logarithm as
\begin{equation}
\log\big|\det(\partial g/\partial y)\big| = n \log(2) + \frac{1}{2} (n + 1) \sum_{i=1}^{n} 2 y_{ii} - \frac{1}{2} (n + 2) \sum_{i=1}^{n} \log\Big( 1 + \sum_{j=1}^{n} y_{ij}^{2}\Big)\,.
\end{equation}

\newpage

\section{Correlation matrix distributions}

Let $X$ be an $n \times n$ real symmetric positive definite matrix
with unit diagonal elements (i.e., an $n \times n$ correlation matrix).

\subsection{Lewandowski-Kurowicka-Joe (LKJ) distribution}

If $X$ follows an
\href{https://mc-stan.org/docs/functions-reference/lkj-correlation.html}{LKJ distribution}
with shape $\eta > 0$,
\begin{equation}
X \sim \mathrm{LKJ}(\eta)\,,
\end{equation}
then its probability density is
\begin{equation}
\begin{aligned}
f(X; \eta)
&= 2^{\sum_{j=1}^{n} (2 \eta - 2 + n - j) (n - j)} \det(X)^{\eta - 1} \prod_{j=1}^{n-1} B(\eta + (n - j - 1)/2, \eta + (n - j - 1)/2)^{n-j} \\
&= 2^{n (n - 1) (\eta - 1 + (2 n - 1)/6)} \det(X)^{\eta - 1} \prod_{j=1}^{n-1} \frac{\Gamma(a(j))^{2 j}}{\Gamma(2 a(j))^{j}}
\end{aligned}
\end{equation}
with logarithm
\begin{equation}
\log(f(X; \eta)) = n (n - 1) (\eta - 1 + (2 n - 1)/6) \log(2) + (\eta - 1) \log(\det(X)) + \sum_{j=1}^{n-1} (2 j \log(\Gamma(a(j))) - j \log(\Gamma(2 a(j))))
\end{equation}
where $a(j) = \eta - 1 + (j + 1)/2$.

\subsection{Computation}

$X$ can be factorized as
\begin{equation}
X = D^{-1/2} L \transpose{L} D^{-1/2}\,,
\end{equation}
where $L$ is a lower triangular matrix with unit diagonal elements
and
\begin{equation}
D = \diag(\diag(L \transpose{L}))\,.
\end{equation}
If the normalization constant can be ignored,
then computing $f(X;\eta)$ amounts to computing $\det(X)$:
\begin{equation}
\begin{aligned}
\det(X)
  &= \det(D^{-1/2}) \det(L) \det(\transpose{L}) \det(D^{-1/2}) \\
  &= \det(D)^{-1/2} \cdot 1 \cdot 1 \cdot \det(D)^{-1/2} \\
  &= \det(D)^{-1}\,,
\end{aligned}
\end{equation}
using the fact that
$\det(A B) = \det(A) \det(B)$ for matrices $A$ and $B$
and
$\det(C) = \prod_{k} (C)_{k,k}$ for triangular matrices $C$.


\section{Covariance matrix distributions}

Let $X$ and $S$ be $n \times n$ real symmetric positive definite matrices
(i.e., $n \times n$ covariance matrices), and let $\nu \in (n - 1, \infty)$.

\subsection{Wishart distribution}

If $X$ follows a
\href{https://mc-stan.org/docs/functions-reference/wishart-distribution.html}{Wishart distribution}
with scale $S$ and degrees of freedom $\nu$,
\begin{equation}
X \sim \mathrm{Wishart}(S, \nu)\,,
\end{equation}
then its probability density function is
\begin{equation}
f(X; S, \nu) = \frac{\det(S)^{-\nu/2} \det(X)^{-(-\nu + n + 1)/2}}{2^{n \nu/2} \Gamma_{n}(\nu/2)} \exp\Big(-\frac{1}{2} \tr(S^{-1} X)\Big)
\end{equation}
with logarithm
\begin{equation}
\log(f(X; S, \nu)) = -\frac{1}{2} (\nu \log(\det(S)) + (-\nu + n + 1) \log(\det(X)) + n \nu \log(2) + 2 \log(\Gamma_{n}(\nu/2)) + \tr(S^{-1} X))\,,
\end{equation}
where $\Gamma_{n}$ denotes the
\href{https://en.wikipedia.org/wiki/Multivariate_gamma_function}{$n$-variate gamma function},
given by
\begin{equation}
\Gamma_{n}(z) = \pi^{n (n - 1)/4} \prod_{i=0}^{n-1} \Gamma\Big(z - \frac{i}{2}\Big)\,,\qquad \Re(z) > \frac{1}{2}(n - 1)\,.
\end{equation}

\subsection{Inverse Wishart distribution}

If $X$ follows an
\href{https://mc-stan.org/docs/functions-reference/inverse-wishart-distribution.html}{inverse Wishart distribution}
with scale $S$ and degrees of freedom $\nu$,
\begin{equation}
X \sim \mathrm{InverseWishart}(S, \nu)\,,
\end{equation}
then its probability density function is
\begin{equation}
f(X; S, \nu) = \frac{\det(S)^{\nu/2} \det(X)^{-(\nu + n + 1)/2}}{2^{n \nu/2} \Gamma_{n}(\nu/2)} \exp\Big(-\frac{1}{2} \tr(S X^{-1})\Big)
\end{equation}
with logarithm
\begin{equation}
\log(f(X; S, \nu)) = -\frac{1}{2} (-\nu \log(\det(S)) + (\nu + n + 1) \log(\det(X)) + n \nu \log(2) + 2 \log(\Gamma_{n}(\nu/2)) + \tr(S X^{-1}))\,.
\end{equation}

\subsection{Computation}

$X$ and $S$ can be factorized as
\begin{equation}
X = D_{\sigma,X} D_{\theta,X}^{-1/2} L_{X} \transpose{L_{X}} D_{\theta,X}^{-1/2} D_{\sigma,X}\,,\qquad S = D_{\sigma,S} D_{\theta,S}^{-1/2} L_{S} \transpose{L_{S}} D_{\theta,S}^{-1/2} D_{\sigma,S}\,,
\end{equation}
where $L_{X}$ and $L_{S}$ are lower triangular matrices
with unit diagonal elements,
\begin{equation}
D_{\sigma,X} = \diag(\diag(X))^{1/2}\,,\qquad D_{\sigma,S} = \diag(\diag(S))^{1/2}\,,
\end{equation}
and
\begin{equation}
D_{\theta,X} = \diag(\diag(L_{X} \transpose{L_{X}}))\,,\qquad D_{\theta,S} = \diag(\diag(L_{S} \transpose{L_{S}}))\,.
\end{equation}

$\det(X)$ is computed as
\begin{equation}
\begin{aligned}
\det(X)
  &= \det(D_{\sigma,X}) \det(D_{\theta,X}^{-1/2}) \det(L_{X}) \det(\transpose{L_{X}}) \det(D_{\theta,X}^{-1/2}) \det(D_{\sigma,X}) \\
  &= \det(D_{\sigma,X}) \det(D_{\theta,X})^{-1/2} \cdot 1 \cdot 1 \cdot \det(D_{\theta,X})^{-1/2} \det(D_{\sigma,X}) \\
  &= \det(D_{\sigma,X})^{2} \det(D_{\theta,X})^{-1}\,,
\end{aligned}
\end{equation}
using the fact that
$\det(A B) = \det(A) \det(B)$ for matrices $A$ and $B$
and
$\det(C) = \prod_{k} (C)_{k,k}$ for triangular matrices $C$.
Similarly,
\begin{equation}
\det(S) = \det(D_{\sigma,S})^{2} \det(D_{\theta,S})^{-1}\,.
\end{equation}

$\tr(S^{-1} X)$ is computed by noting that
\begin{equation}
\tr(A B) = \sum_{i,j} (A \odot \transpose{B})_{i,j} = \sum_{i,j} (\transpose{A} \odot B)_{i,j}\,,
\end{equation}
where $\odot$ denotes elementwise multiplication.
Then
\small
\begin{equation}
\begin{aligned}
\tr(S^{-1} X)
  &= \sum_{i,j} (S^{-1} \odot \transpose{X})_{i,j} \\
  &= \sum_{i,j} (S^{-1} \odot X)_{i,j} \\
  &= \sum_{i,j} (D_{\sigma,S}^{-1} D_{\theta,S}^{1/2} \transpose{(L_{S}^{-1})} L_{S}^{-1} D_{\theta,S}^{1/2} D_{\sigma,S}^{-1} \odot D_{\sigma,X} D_{\theta,X}^{-1/2} L_{X} \transpose{L_{X}} D_{\theta,X}^{-1/2} D_{\sigma,X})_{i,j} \\
  &= \sum_{i,j} (D_{1} V_{1} \transpose{D_{1}})_{i,j}\,,
\end{aligned}
\end{equation}
\normalsize
where
\begin{equation}
V_{1} = \transpose{(L_{S}^{-1})} L_{S}^{-1} \odot L_{X} \transpose{L_{X}}\,,\qquad D_{1} = D_{\sigma,S}^{-1} D_{\theta,S}^{1/2} D_{\sigma,X} D_{\theta,X}^{-1/2}\,.
\end{equation}

Similarly,
\begin{equation}
\begin{aligned}
\tr(S X^{-1})
  &= \sum_{i,j} (\transpose{S} \odot X^{-1})_{i,j} \\
  &= \sum_{i,j} (S \odot X^{-1})_{i,j} \\
  &= \sum_{i,j} (D_{\sigma,S} D_{\theta,S}^{-1/2} L_{S} \transpose{L_{S}} D_{\theta,S}^{-1/2} D_{\sigma,S} \odot D_{\sigma,X}^{-1} D_{\theta,X}^{1/2} \transpose{(L_{X}^{-1})} L_{X}^{-1} D_{\theta,X}^{1/2} D_{\sigma,X}^{-1})_{i,j} \\
  &= \sum_{i,j} (D_{2} V_{2} \transpose{D_{2}})_{i,j}\,,
\end{aligned}
\end{equation}
where
\begin{equation}
V_{2} = L_{S} \transpose{L_{S}} \odot \transpose{(L_{X}^{-1})} L_{X}^{-1}\,,\qquad D_{2} = D_{\sigma,S} D_{\theta,S}^{-1/2} D_{\sigma,X}^{-1} D_{\theta,X}^{1/2}\,.
\end{equation}

\end{document}
